#kafka.bootstrap-servers=localhost:9092
#kafka.consumer.group-id=test-consumer-group
#kafka.consumer.auto-offset-reset=earliest
#kafka.consumer.concurrency=1
#kafka.autocommit.interval_ms=1000
#kafka.enable.autocommit=false
#kafka.session.timeout_ms=15000
#kafka.topic.event=topic-study-mq
#spring.kafka.consumer.max-poll-records=1
server.port=8082
#kafka config
#acks:消息的确认机制，默认值是0， acks=0：如果设置为0，生产者不会等待kafka的响应。 acks=1：这个配置意味着kafka会把这条消息写到本地日志文件中，但是不会等待集群中其他机器的成功响应。 acks=all：这个配置意味着leader会等待所有的follower同步完成。这个确保消息不会丢失，除非kafka集群中所有机器挂掉。这是最强的可用性保证。
spring.kafka.producer.acks=all
#发送失败重试次数，配置为大于0的值的话，客户端会在消息发送失败时重新发送。
spring.kafka.producer.retries=30
#当多条消息需要发送到同一个分区时，生产者会尝试合并网络请求。这会提高client和生产者的效率。
spring.kafka.producer.batch-size=16384
#即32MB的批处理缓冲区
spring.kafka.producer.buffer-memory=33554432
spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerializer
spring.kafka.producer.value-serializer=org.springframework.kafka.support.serializer.JsonSerializer
#//往kafka服务器提交消息间隔时间，0则立即提交不等待
spring.kafka.producer.properties.linger.ms=1
spring.kafka.consumer.group-id=test-consumer-group
#earliest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，从头开始消费
#latest:当各分区下有已提交的offset时，从提交的offset开始消费；无提交的offset时，消费新产生的该分区下的数据
#none：topic各分区都存在已提交的offset时，从offset后开始消费；只要有一个分区不存在已提交的offset，则抛出异常
spring.kafka.consumer.auto-offset-reset=earliest
#如果为true，消费者的偏移量将在后台定期提交
spring.kafka.consumer.enable-auto-commit=false
spring.kafka.consumer.max-poll-records=100
spring.kafka.consumer.key-deserializer=org.apache.kafka.common.serialization.StringDeserializer
spring.kafka.consumer.value-deserializer=org.apache.kafka.common.serialization.StringDeserializer
#超过这个时间就摘掉消费者
spring.kafka.consumer.properties.session.timeout.ms=15000
spring.kafka.bootstrap-servers=localhost:9092
#并行消费
spring.kafka.listener.concurrency=3
spring.kafka.listener.ack-mode=manual_immediate